<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Case Studies on Sin(x)</title>
    <link>https://sinanthahir.github.io/tags/case-studies/</link>
    <description>Recent content in Case Studies on Sin(x)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://sinanthahir.github.io/tags/case-studies/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Machine Learning for Recognizing Handwritten Digits</title>
      <link>https://sinanthahir.github.io/blogs/recognizing-handwritten-digits/</link>
      <pubDate>Thu, 04 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://sinanthahir.github.io/blogs/recognizing-handwritten-digits/</guid>
      <description>&lt;h2 id=&#34;machine-learning-for-recognizing-handwritten-digits&#34;&gt;Machine Learning for Recognizing Handwritten Digits&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;00head.jpeg&#34; alt=&#34;cover&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Machine learning&lt;/strong&gt; is a field of &lt;em&gt;artificial intelligence&lt;/em&gt; in which a system is designed to learn automatically given a set of input data. After the system has learnt (we say that the system has been trained), we can use it to make predictions for new data, unseen before. This approach makes it possible to solve complex problems which are difficult or impossible to solve with traditional sequential programming.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recognizing handwritten text&lt;/strong&gt; is a problem that traces back to the first automatic machines that needed to recognize individual characters in handwritten documents. Think about, for example, the ZIP codes on letters at the post office and the automation needed to recognize these five digits. Perfect recognition of these codes is necessary to sort mail automatically and efficiently. Included among the other applications that may come to mind is &lt;strong&gt;OCR (Optical Character Recognition)&lt;/strong&gt; software. OCR software must read handwritten text, or pages of printed books, for general electronic documents in which each character is well defined. But the problem of handwriting recognition goes farther back in time, more precisely to the early 20th Century (the 1920s), when Emanuel Goldberg (1881–1970) began his studies regarding this issue and suggested that a statistical approach would be an optimal choice.&lt;/p&gt;






    
    


&lt;div class=&#34;rounded p-3 my-6&#34; style=&#34;background-color: #d9edf7; color: #31708f;&#34; &gt;
    
In this project, our goal is to get started hands on with machine learning to recognize this handwrittens, so I’m only going to give you a simplified explanation for now.

&lt;/div&gt;

&lt;p&gt;&lt;img src=&#34;train-network.png&#34; alt=&#34;network&#34;&gt;&lt;/p&gt;
&lt;p&gt;The above network displays a network of successive training sets, which consists of &lt;em&gt;the image of a digit &amp;amp; a label, which tells us what the image truly represents.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;At first, the first image is processed by the neural network and produces an answer: &lt;code&gt;that is a 9&lt;/code&gt;. The connections of neurons in the network be in randomly stated and is providing anything useful, just a random answer.&lt;/p&gt;
&lt;p&gt;The answer is compared to the label. Here, the value of &lt;code&gt;(9)&lt;/code&gt; is actually different from the label, i.e the value &lt;code&gt;(3)&lt;/code&gt;. Some feedback is given back such that the network can improve, favoring in tend to give a correct answer.&lt;/p&gt;
&lt;p&gt;Then the next example are considered and the neural network learns in such iterative way.&lt;/p&gt;
&lt;h2 id=&#34;aim&#34;&gt;Aim:&lt;/h2&gt;
&lt;p&gt;The primary aim of this project involves predicting a numeric value, and then reading and interpreting an image that uses a handwritten font.&lt;/p&gt;
&lt;h2 id=&#34;hypothesis&#34;&gt;Hypothesis:&lt;/h2&gt;
&lt;p&gt;The Digits data set of the scikit-learn library provides numerous datasets that are useful for testing many problems of data analysis and prediction of the results. Some Scientist claims that it predicts the digit accurately 95% of the times. Perform data Analysis to accept or reject this Hypothesis.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Here, I will be using Anaconda Kernel Interpreted VS Code Environment and using Python libraries like Matplotlib, Seaborn, Scikit-Learn.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;the-digits-dataset&#34;&gt;The Digits Dataset:&lt;/h2&gt;
&lt;p&gt;The scikit-learn library provides many datasets that are useful for testing many problems of data analysis and prediction of the results. Also in this case there is a dataset of images called Digits. This dataset comprises 1,797 images that are 8x8 pixels in size. Each image is a handwritten digit in grayscale.&lt;/p&gt;
&lt;h3 id=&#34;importing-dataset&#34;&gt;Importing Dataset&lt;/h3&gt;
&lt;p&gt;Import datasets module from sklearn library and load the digits dataset using the &lt;code&gt;load_digits()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;from sklearn import datasets
digits = datasets.load_digits()
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;description-of-dataset&#34;&gt;Description of Dataset&lt;/h3&gt;
&lt;p&gt;After loading the dataset, we can read the information about the dataset by calling the &lt;code&gt;DESCR&lt;/code&gt; attribute.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;print(digits.DESCR)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The textual description of the dataset, the authors who contributed to its creation, and the references will appear as shown in the output.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;01.png&#34; alt=&#34;desc_out&#34;&gt;&lt;/p&gt;
&lt;p&gt;Each dataset in the scikit-learn library has a field containing all the information.&lt;/p&gt;
&lt;h3 id=&#34;targets&#34;&gt;Targets&lt;/h3&gt;
&lt;p&gt;The numerical values represented by images, i.e., the targets, are contained in the &lt;code&gt;digit.targets&lt;/code&gt; array.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;digits.target
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Output:
array([0, 1, 2, ..., 8, 9, 8])
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;dataset-shape&#34;&gt;Dataset Shape&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Dimensions&lt;/strong&gt; of the dataset can be obtained using &lt;code&gt;data.shape()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;digits.data.shape
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Output:
(1797, 64)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The output shows that the dataset has 1797 images of 8x8 size(i.e, 8 * 8 = 64 px). In other words, this array could be represented in 3D as a pile of images with 8x8 pixels each.&lt;/p&gt;
&lt;h3 id=&#34;images-of-the-handwritten-digits&#34;&gt;Images of the handwritten digits&lt;/h3&gt;
&lt;p&gt;The images of the handwritten digits are contained in an array. Each element of this array is an image that is represented by an 8x8 matrix of numerical values that correspond to grayscale from white, with a value of 0, to black, with the value 15.&lt;/p&gt;
&lt;p&gt;Let’s look at the data of the first 8x8 image. Each slot in the array corresponds to a pixel, and the value in the slot is the amount of black in the pixel.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;digits.images[0]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;dig-image.png&#34; alt=&#34;digits&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;visualization-of-an-array&#34;&gt;Visualization of an array&lt;/h3&gt;
&lt;p&gt;We can visually check the contents of this result. The following steps can get it done&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Import &lt;code&gt;pyplot&lt;/code&gt; module which is under matplotlib as &lt;code&gt;plt&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;imshow()&lt;/code&gt; function is used to display data as an image; i.e. on a 2D regular raster.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cmap = gray_r&lt;/code&gt; displays a grayscale image.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;interpolation= ‘nearest’&lt;/code&gt; displays an image without trying to interpolate between pixels if the display resolution is not the same as the image resolution.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;title()&lt;/code&gt; function is used to display the title on the graph.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import matplotlib.pyplot as plt
plt.imshow(digits.images[0], cmap=plt.cm.gray_r, interpolation=&#39;nearest&#39;)
plt.title(&#39;one of the 1797 handwritten digits&#39;)
plt.savefig(&#39;plot1.png&#39;, dpi=100, bbox_inches=&#39;tight&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;By running this command, we will obtain the grayscale image as follows:
&lt;img src=&#34;gigi01.png&#34; alt=&#34;sing-digi&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;visualization-of-10-digits&#34;&gt;Visualization of 10 digits&lt;/h3&gt;
&lt;p&gt;Using the NumPy and matplotlib libraries, we can display each digit from 0 to 9 which are in the form of an array as images.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;figure()&lt;/code&gt; function in the pyplot module of the matplotlib library is used to create a new &lt;strong&gt;figure&lt;/strong&gt; with a specified size of (15,4).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;subplots_adjust(hspace=0.8)&lt;/code&gt; is used to adjust the space between the rows of the subplots.&lt;/li&gt;
&lt;li&gt;Combine two lists using the &lt;code&gt;zip()&lt;/code&gt; function for easier handling inside the plotting loop.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;enumerate()&lt;/code&gt; method adds a counter to an iterable and returns it. The returned object is a &lt;code&gt;enumerate&lt;/code&gt; object.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;subplot()&lt;/code&gt; function is used to add a subplot to a current figure at the specified grid position.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import numpy as np 
plt.figure(figsize=(15,4))
plt.subplots_adjust(hspace=0.8)images_and_labels = list(zip(digits.images, digits.target)) 
for index, (image, label) in enumerate(images_and_labels[:10]):                                    
    plt.subplot(2, 5, index + 1)    
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation=&#39;nearest&#39;) 
    plt.title(&#39;Training: %i&#39; % label, fontsize =12)
plt.savefig(&#39;plot2.png&#39;, dpi=300, bbox_inches=&#39;tight&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;alldigi.png&#34; alt=&#34;all&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;flatten-the-input-images&#34;&gt;Flatten the input images&lt;/h3&gt;
&lt;p&gt;The inputs are 8x8 grayscale images. we can produce a flat array of 64-pixel values so that each pixel corresponds to a column for the classifier.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;len()&lt;/code&gt; function gives the number of images in the dataset.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;reshape()&lt;/code&gt; function returns an array containing the same data with a new shape.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;n = len(digits.images)
print(n)
data = digits.images.reshape((n, -1))
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Output:
1797
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It was reported that the dataset is a training set consisting of 1,797 images. We determined that it is true.&lt;/p&gt;
&lt;h3 id=&#34;the-machine-learning-model&#34;&gt;The Machine Learning Model&lt;/h3&gt;
&lt;p&gt;An estimator that is useful in this case is sklearn.svm.SVC, which uses the technique of &lt;strong&gt;Support Vector Classification (SVC)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;“Support Vector Machine” (SVM)&lt;/strong&gt; is a supervised machine learning algorithm that is mostly used in classification problems.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can read more about SVM model from &lt;a href=&#34;https://scikit-learn.org/stable/modules/svm.html&#34;&gt;Scikit-Learn’s Official Document&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Import the SVM module of the scikit-learn library and create an estimator of SVC type and then choose an initial setting, assigning the values C and gamma generic values.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#import svm model
from sklearn import svm

#Create a SVMClassifier
svc = svm.SVC(gamma=0.001, C=100.)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;split-the-dataset&#34;&gt;Split the Dataset&lt;/h3&gt;
&lt;p&gt;once we define a predictive model, we must instruct it with a training and test set. The &lt;strong&gt;training set&lt;/strong&gt; is a set of data in which you already know the belonging class and the &lt;strong&gt;test set&lt;/strong&gt; is a secondary data set that is used to test a machine learning program after it has been trained on initial training.&lt;/p&gt;
&lt;p&gt;Import &lt;code&gt;train_test_split()&lt;/code&gt; function which is used for splitting data arrays into two subsets i.e., into train and test sets.&lt;/p&gt;
&lt;p&gt;Here we have split the data by assigning &lt;strong&gt;0.01&lt;/strong&gt; as test size.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.01, random_state=0)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;train-the-model&#34;&gt;Train the model&lt;/h3&gt;
&lt;p&gt;we can train the svc estimator that we defined earlier using the &lt;code&gt;fit()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;After a short time, the trained estimator will appear with text output.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;svc.fit(x_train, y_train)
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Output:
SVC(C=100.0, gamma=0.001)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;test-the-model&#34;&gt;Test the model&lt;/h3&gt;
&lt;p&gt;we can test our estimator by making it interpret the digits of the test set using &lt;code&gt;predict()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;y_pred = svc.predict(x_test)
y_pred
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Output:
array([2, 8, 2, 6, 6, 7, 1, 9, 8, 5, 2, 8, 6, 6, 6, 6, 1, 0])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We obtain the results in the form of an array.&lt;/p&gt;
&lt;h3 id=&#34;visualize-the-test-images&#34;&gt;Visualize the test images&lt;/h3&gt;
&lt;p&gt;We can plot the images of the predicted digits from the array using the following code.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;images_and_predictions = list(zip(x_test,y_pred))
plt.figure(figsize=(18,5))
for index, (image, prediction) in enumerate(images_and_predictions[:19]):
     plt.subplot(2, 9, index + 1)
     image = image.reshape(8, 8)
     plt.imshow(image, cmap=plt.cm.gray_r, interpolation=&#39;nearest&#39;)
     plt.title(&#39;Prediction: %i&#39; % prediction)
# save the figure
plt.savefig(&#39;plot3.png&#39;, dpi=300, bbox_inches=&#39;tight&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;res.png&#34; alt=&#34;result&#34;&gt;
It is able to recognize the handwritten digits and interprete all the digits of the validation set correctly.&lt;/p&gt;
&lt;h3 id=&#34;accuracy-of-the-model&#34;&gt;Accuracy of the model&lt;/h3&gt;
&lt;p&gt;The accuracy score of the model can be obtained using the &lt;code&gt;score()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;score = svm.score(x_test, y_test)
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Output:
Accuracy Score: 1.0
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;confusion-matrix&#34;&gt;Confusion Matrix&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;confusion matrix&lt;/strong&gt; is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#For Confusion Matrix
from sklearn.metrics import confusion_matrix
import pandas as pd
import seaborn as sn 

data = confusion_matrix(y_test, y_pred)
df_cm = pd.DataFrame(data, columns=np.unique(y_test), index = np.unique(y_test))
df_cm.index.name = &#39;Actual&#39;
df_cm.columns.name = &#39;Predicted&#39; 

plt.figure(figsize = (10,10))
sn.set(font_scale=1.4)#for label size
plt.title(&#39;Confusion Matrix&#39;)
sn.heatmap(df_cm, annot=True,annot_kws={&amp;quot;size&amp;quot;: 12})# font size
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;conf-mat.png&#34; alt=&#34;confusion&#34;&gt;
A &lt;strong&gt;Classification report&lt;/strong&gt; is used to measure the quality of predictions from a &lt;strong&gt;classification&lt;/strong&gt; algorithm.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;clas-repo.png&#34; alt=&#34;report&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Given the large number of elements contained in the Digits dataset, we will certainly obtain a very effective model, i.e., one that’s capable of recognizing with good certainty.&lt;/p&gt;
&lt;p&gt;We test the hypothesis by using these cases, each case for a different range of training and validation sets.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;tab11.png&#34; alt=&#34;tabular&#34;&gt;&lt;/p&gt;
&lt;p&gt;After performing the data analysis on the dataset with three different test cases, we can conclude that the given hypothesis is true i.e., the model predicts the digit accurately 95% of the times.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can find Source code: &lt;a href=&#34;https://github.com/sinanthahir/Internship_suven_technology/blob/main/Recognizing-Handwritten-Digits/handwritten-digits-recognizer.ipynb&#34;&gt;github@sinanthahir&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Experience Analysis through Weather Data : Exploratory Data Analysis</title>
      <link>https://sinanthahir.github.io/blogs/exp-weather-eda/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://sinanthahir.github.io/blogs/exp-weather-eda/</guid>
      <description>&lt;h2 id=&#34;experience-analysis-through-weather-data--exploratory-data-analysis&#34;&gt;Experience Analysis through Weather Data : Exploratory Data Analysis&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;cover.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;Knowing accurate weather conditions is an important element for individuals as well as organizations. Many businesses rely on weather conditions. It is necessary to have the correct data to get accurate decisions. One type of data that’s easier to find on the internet is Weather data. Many sites provide historical data on many meteorological parameters.&lt;/p&gt;
&lt;p&gt;Exploratory Data Analysis is an approach to analyze data, to summarize the main characteristics of data, and better understand the data set. It also allows us to quickly interpret the data and adjust different variables to see their effect. The three main steps to get a perfect EDA are &lt;code&gt;extracting&lt;/code&gt; the data from an authorized source, &lt;code&gt;cleaning&lt;/code&gt; and &lt;code&gt;processing&lt;/code&gt; the data, and performing data &lt;code&gt;visualization&lt;/code&gt; on the cleaned data set.&lt;/p&gt;
&lt;p&gt;Here, I will work through out a practical exploratory data analysis which was done a part of my data analytics internship at &lt;a href=&#34;https://suvenconsultants.com/&#34;&gt;Suven Consultants &amp;amp; Technology&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;objective&#34;&gt;Objective:&lt;/h3&gt;
&lt;p&gt;The main focus of our project was to perform analysis for testing the Influences of Global Warming and finally put forth a conclusion.&lt;/p&gt;
&lt;h3 id=&#34;hypothesis&#34;&gt;Hypothesis:&lt;/h3&gt;
&lt;p&gt;A hypothesis is an assumption, an idea that is proposed for the sake of argument so that it can be tested to see if it might be true.






    
    


&lt;div class=&#34;rounded p-3 my-6&#34; style=&#34;background-color: #d9edf7; color: #31708f;&#34; &gt;
    
The Null Hypothesis H0 is “Has the Apparent temperature and humidity compared monthly across 10 years of the data indicate an increase due to Global warming”

&lt;/div&gt;

That means we need to find whether the average Apparent temperature for the month of a month says April starting from 2006 to 2016 and the average humidity for the same period have increased or not.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So, What is this Apparent Temperature and Humidity mentioned in the Null Hypothesis (H0)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These are called Terminologies, or rather say the column names or criteria used to constrain the data we have to different specification or class. In order to know that we must look up for basic terminologies used in the data we are working on.&lt;/p&gt;
&lt;h3 id=&#34;terminologies&#34;&gt;Terminologies:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Meteorological Data&lt;/strong&gt; refers to data consisting of physical parameters that are measured directly by instrumentation, and include temperature, dew point, wind direction, wind speed, cloud cover, cloud layer(s), ceiling height, visibility, current weather, and precipitation amount.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Apparent temperature&lt;/strong&gt; is the temperature equivalent perceived by humans, caused by the combined effects of air temperature, relative humidity, and wind speed. The measure is most commonly applied to the perceived outdoor temperature.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Humidity&lt;/strong&gt; is the amount of water vapor in the air. If there is a lot of water vapor in the air, the humidity will be high. The higher the humidity, the wetter it feels outside.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can check out more weather terminologies from &lt;a href=&#34;https://kestrelmeters.com/pages/weather-glossary&#34;&gt;Kestrelmeter’s Glossary&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset:&lt;/h3&gt;
&lt;p&gt;The dataset currently using, can be obtained from &lt;a href=&#34;https://www.kaggle.com/muthuj7/weather-dataset&#34;&gt;Kaggle&lt;/a&gt;. The dataset has hourly temperature recorded for the last 10 years starting from 2006–04–01 00:00:00.000 +0200 to 2016–09–09 23:00:00.000 +0200. It corresponds to Finland, a country in Northern Europe.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Now, we have our Objective, Dataset, and basic understanding of the Terminologies. So let’s start of journey to analyze the data!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;data-preprocessing&#34;&gt;Data Preprocessing&lt;/h3&gt;
&lt;p&gt;Here, i’m using Anaconda Environment with Visual Studio Code. You can also set up such a system which enable a faster git and pipeline integration.&lt;/p&gt;
&lt;h4 id=&#34;importing-required-libraries&#34;&gt;Importing required libraries:&lt;/h4&gt;
&lt;p&gt;We will be using Python libraries such as Pandas, Numpy, Matplotlib and Seaborn.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;loading-dataset&#34;&gt;Loading dataset:&lt;/h4&gt;
&lt;p&gt;Load the dataset using &lt;code&gt;read_csv()&lt;/code&gt; function as the dataset is in CSV form and read the first 5 rows from data using &lt;code&gt;head()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;data = pd.read_csv(&#39;weatherHistory.csv&#39;)
data.head()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;01.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dimensions&lt;/strong&gt; of the dataframe refers to the overall data sample, i.e, the total number of rows and columns in the data. It can be obtained using &lt;code&gt;data.shape&lt;/code&gt; function as follows&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;data.shape
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The total number of rows and columns in the data set is &lt;strong&gt;96453&lt;/strong&gt; and &lt;strong&gt;12&lt;/strong&gt; respectively.&lt;/p&gt;
&lt;p&gt;To find out the types and overall summary of the data frame, we use the &lt;code&gt;data.info()&lt;/code&gt; function. It comes in handy when doing exploratory analysis of the data.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;data.info()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;00.png&#34; alt=&#34;info_output&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can use the &lt;code&gt;describe()&lt;/code&gt; function to get the descriptive statistical details of the data-frame.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;data.describe()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;03.png&#34; alt=&#34;stat&#34;&gt;&lt;/p&gt;
&lt;p&gt;To check the distinct elements in the data frame, we can use the &lt;code&gt;nunique()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;data.nunique()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;04.png&#34; alt=&#34;nunique&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can check for any missing values using the &lt;code&gt;isnull()&lt;/code&gt; function. since having a large dataset, we can incorporate &lt;code&gt;sum()&lt;/code&gt; function to get the total number of missing value in each columns.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;data.isnull().sum()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;05.png&#34; alt=&#34;isnull&#34;&gt;&lt;/p&gt;
&lt;p&gt;Till now, we had got up many details about the dataset we are working on. Lets take it into account.&lt;/p&gt;
&lt;h3 id=&#34;observations&#34;&gt;Observations:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;In ‘&lt;em&gt;Precip Type&lt;/em&gt;’, there are 517 missing values.&lt;/li&gt;
&lt;li&gt;‘&lt;em&gt;Wind Bearing (degrees)&lt;/em&gt;’ has only integer values.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formatted Date&lt;/strong&gt; is in String.&lt;/li&gt;
&lt;li&gt;Minimum values of Humidity, Wind Speed (km/h), Wind Bearing (degrees), Visibility (km) are &lt;strong&gt;Zero&lt;/strong&gt; and they can be &lt;strong&gt;Zero&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;From Statistical details and Distinct Elements in Dataframe, It is noticed that ’Loud Cover’ are &lt;strong&gt;zero&lt;/strong&gt; or &lt;strong&gt;null&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can &lt;em&gt;remove the unwanted columns&lt;/em&gt; which don’t add value to the analysis using the &lt;strong&gt;drop()&lt;/strong&gt; function. We will drop ‘Loud Cover’ as it has one unique value 0 and it is not useful in analysis.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;data = data.drop([‘Loud Cover’], axis = 1)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;correlation-of-the-columns&#34;&gt;Correlation of the Columns:&lt;/h3&gt;
&lt;p&gt;Correlation matrices are an essential tool of exploratory data analysis. We can display the pairwise correlation using &lt;code&gt;corr()&lt;/code&gt; function which creates the correlation matrix between all the features in the dataset. &lt;strong&gt;Correlation heatmaps&lt;/strong&gt; contain the same information in a visually appealing way.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# assign data correlation matrix
relation = data.corr()

# Increase the size of the heatmap
plt.figure(figsize=(10,8))

# Store heatmap object in a variable to easily access it
when you want to include more features and you can set 
the annotation parameter to True to display the 
correlation values on the heatmap.
sns.heatmap(data=relation)

# Give a title to the heatmap.
plt.title(&amp;quot;Correlation of columns in the dataframe&amp;quot;)

# save the figure.
plt.savefig(&#39;plot1.png&#39;, dpi=300, bbox_inches=&#39;tight&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;06.png&#34; alt=&#34;heatmap&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;observation&#34;&gt;Observation:&lt;/h3&gt;
&lt;p&gt;From the Pairwise correlation chart, we can see that &lt;em&gt;Apparent Temperature&lt;/em&gt; and &lt;em&gt;Humidity&lt;/em&gt; have a high degree of correlation with each other. So we have a high chance of validating our hypothesis.&lt;/p&gt;
&lt;p&gt;We only need 3 columns for checking and validating our task which is data &lt;strong&gt;[‘Formatted Date’, ‘Apparent Temperature(c)’, ‘Humidity’]&lt;/strong&gt;. So, we can ignore other columns and missing values.&lt;/p&gt;
&lt;h3 id=&#34;parsing-dates-creating-new-dataframe-&#34;&gt;Parsing Dates, Creating new dataframe :&lt;/h3&gt;
&lt;p&gt;Change the ‘Formatted Date’ feature from String to Datetime using the &lt;code&gt;datetime()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;data[‘Formatted Date’] = pd.to_datetime(data[‘Formatted Date’],utc=True)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can set “Formatted Date” as an index using the &lt;code&gt;set_index()&lt;/code&gt; function which sets the DataFrame index (row labels) using one or more existing columns.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;data = data.set_index(“Formatted Date”)
data.info()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;07.png&#34; alt=&#34;parsed&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;resampling-data&#34;&gt;Resampling Data:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Resampling&lt;/strong&gt; is a convenient method for frequency conversion. The object must have a datetime like an index.&lt;/p&gt;
&lt;p&gt;Now, we have hourly data, we need to resample it to monthly. We only require the Apparent Temperature and humidity columns to test the hypothesis. So, we will consider these two columns and perform a &lt;code&gt;resample()&lt;/code&gt; function from Pandas.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;df_column = [&#39;Apparent Temperature (C)&#39;, &#39;Humidity&#39;]
df_monthly_mean = data[df_column].resample(&amp;quot;MS&amp;quot;).mean() 
#MS-Month Starting
df_monthly_mean.head()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now, we converts hourly data to monthly data using “&lt;strong&gt;MS&lt;/strong&gt;” which denotes the Month starting. We are displaying the average apparent temperature and humidity using the &lt;strong&gt;mean()&lt;/strong&gt; function.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;08.png&#34; alt=&#34;resample&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We are done with cleaning and resampling it. Now, Lets begin our analysis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;relation-between-apparent-temperature--humidity-using-regression&#34;&gt;Relation between Apparent Temperature &amp;amp; Humidity Using Regression:&lt;/h2&gt;
&lt;p&gt;We can use the &lt;code&gt;regplot()&lt;/code&gt; function to plot the relationship between the “Apparent Temperature ” and “Humidity”.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# calling regplot function and assign it with our data and plot labels and color parameter.
sns.regplot(data=df_monthly_mean, x=&amp;quot;Apparent Temperature (C)&amp;quot;, y=&amp;quot;Humidity&amp;quot;, color=&amp;quot;r&amp;quot;)

# Give a title to the plot
plt.title(&amp;quot;Relation between Apparent Temperature (C) and Humidity&amp;quot;)

# save the figure
plt.savefig(&#39;plot2.png&#39;, dpi=300, bbox_inches=&#39;tight&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;09.png&#34; alt=&#34;regplot&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;observation-1&#34;&gt;Observation:&lt;/h3&gt;
&lt;p&gt;There is a Linear Relation between “&lt;em&gt;Apparent Temperature&lt;/em&gt; ” and “&lt;em&gt;Humidity&lt;/em&gt;” with a &lt;strong&gt;negative&lt;/strong&gt; slope.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;As air temperature increases, air can hold more water molecules, and its relative humidity decreases. When temperatures drop, relative humidity increases.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;yearly-variation-of-apparent-temperature-and-humidity&#34;&gt;Yearly Variation of Apparent Temperature and Humidity:&lt;/h2&gt;
&lt;p&gt;We use &lt;code&gt;lineplot()&lt;/code&gt; function to plot the Variation of Apparent Temperature and Humidity with time.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;plt.figure(figsize=(15,7))
sns.lineplot(data= df_monthly_mean)
plt.xlabel(&#39;year&#39;)
plt.title(&#39;Variation of Apparent Temprature and HUmidity with Time&#39;)
plt.savefig(&#39;plot3.png&#39;, dpi=300, bbox_inches=&#39;tight&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;10.png&#34; alt=&#34;var&#34;&gt;&lt;/p&gt;
&lt;p&gt;The above graph displays average temperature and humidity for all 12 months over the 10 years i.e., from 2006 to 2016.&lt;/p&gt;
&lt;h3 id=&#34;observation-2&#34;&gt;Observation:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;“&lt;em&gt;Humidity&lt;/em&gt;” remained constant from 2006–2016&lt;/li&gt;
&lt;li&gt;“&lt;em&gt;Apparent Temperature&lt;/em&gt;” changed from 2006–2016 at regular intervals with constant amplitude.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;variation-of-humidity--apparent-temperature-for-all-months&#34;&gt;Variation of Humidity &amp;amp; Apparent Temperature for all months:&lt;/h2&gt;
&lt;p&gt;Creating a function which labels each month number with actual &lt;strong&gt;month name&lt;/strong&gt; and a &lt;em&gt;specified color&lt;/em&gt; for graph. and defining a seaborn &lt;code&gt;plot()&lt;/code&gt; function. This function helps to analyze the variations in Apparent Temperature and Humidity for all months over the 10 years.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Defining a function call for month to be labeled
def label_color(month):
    if month == 1:
       return &#39;January&#39;,&#39;black&#39;
    elif month == 2:
       return &#39;February&#39;,&#39;brown&#39;
    elif month == 3:
       return &#39;March&#39;,&#39;red&#39;
    elif month == 4:
       return &#39;April&#39;,&#39;orange&#39;
    elif month == 5:
       return &#39;May&#39;,&#39;yellow&#39;
    elif month == 6:
       return &#39;June&#39;,&#39;blue&#39;
    elif month == 7:
       return &#39;July&#39;,&#39;violet&#39;
    elif month == 8:
       return &#39;August&#39;,&#39;pink&#39;
    elif month == 9:
       return &#39;September&#39;,&#39;grey&#39;
    elif month == 10:
       return &#39;October&#39;,&#39;pink&#39;
    elif month == 11:
       return &#39;November&#39;,&#39;purple&#39;
    else:
       return &#39;December&#39;,&#39;green&#39;

# Assigning variables to resampled data
TEMP_DATA = df_monthly_mean.iloc[:,0]
HUM_DATA = df_monthly_mean.iloc[:,1]
def plot_month(month, data):
    label, color = label_color(month)
    mdata = data[data.index.month == month]
    sns.lineplot(data=mdata,label=label,color=color,marker=&#39;o&#39;)
def sns_plot(title, data):
    plt.figure(figsize=(14,8))
    plt.title(title)
    plt.xlabel(&#39;YEAR&#39;)
    for i in range(1,13):
        plot_month(i,data)
    plt.savefig(&#39;plot4.png&#39;, dpi=300, bbox_inches=&#39;tight&#39;)
    plt.show()

# Month-wise Plot for Apparent Temperature of 10 years 
title = &#39;Month-wise Plot for Apparent Temperature of 10 years&#39; 
sns_plot(title, TEMP_DATA)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;11.png&#34; alt=&#34;color&#34;&gt;
This graph shows the changes in Temperature for each month from 2006 to 2016.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Month-wise Plot for Humidity of 10 years 
title = &#39;Month-wise Plot for Humidity of 10 years&#39; 
sns_plot(title, HUM_DATA)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;12.png&#34; alt=&#34;humcolor&#34;&gt;&lt;/p&gt;
&lt;p&gt;This graph shows the changes in Humidity for each month from 2006 to 2016.&lt;/p&gt;
&lt;h2 id=&#34;variation-of-humidity--apparent-temperature-for-each-months&#34;&gt;Variation of Humidity &amp;amp; Apparent Temperature for each months:&lt;/h2&gt;
&lt;p&gt;Creating a function that helps to analyze the variations in Apparent Temperature and Humidity for each month over the 10 years.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Function for plotting variation for each month
def sns_month_plot(month):
    plt.figure(figsize=(15,7))
    label = label_color(month)[0]
    plt.title(&#39;Apparent Temperature Vs Humidity for {}&#39;.format(label))
    data = df_monthly_mean[df_monthly_mean.index.month == month]
    plt.xlabel(&#39;YEAR&#39;)
    sns.lineplot(data=data, marker=&#39;o&#39;)
    name=&amp;quot;month&amp;quot;+str(month)+&amp;quot;.png&amp;quot;
    plt.savefig(name, dpi=300, bbox_inches=&#39;tight&#39;)
    plt.show()

# Plot for the month of &#39;January - December&#39;
for month in range(1,13):
    sns_month_plot(month)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The graphs below show the variations in Apparent Temperature and Humidity for each month from 2006 to 2016.&lt;/p&gt;
&lt;h4 id=&#34;january&#34;&gt;January&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;13.png&#34; alt=&#34;jan&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;february&#34;&gt;February&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;14.png&#34; alt=&#34;feb&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;march&#34;&gt;March&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;15.png&#34; alt=&#34;mar&#34;&gt;&lt;/p&gt;
&lt;p&gt;And so on.&lt;/p&gt;
&lt;h3 id=&#34;observation-3&#34;&gt;Observation:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;As from the above plots, we can understand that, ’The Apparent Temperature’ has a tremendous fluctuation over the time period.&lt;/li&gt;
&lt;li&gt;There is a sharp rise of temperature between year 2008–2009 which again decreases in year 2009–2010.&lt;/li&gt;
&lt;li&gt;It is observed that the average Apparent Temperature is at its peak in year 2009 which further drops to its lowest in year 2015.&lt;/li&gt;
&lt;li&gt;Whereas the average Humidity has remained nearly constant over the period of time.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion:&lt;/h3&gt;
&lt;p&gt;From this analysis, We can conclude that the Apparent temperature and humidity compared monthly across 10 years of the data indicate an increase due to Global warming. This clears that our &lt;em&gt;Null Hypothesis&lt;/em&gt; is having a &lt;strong&gt;True positive&lt;/strong&gt; impact.&lt;/p&gt;
&lt;h3 id=&#34;miscellaneous&#34;&gt;Miscellaneous:&lt;/h3&gt;
&lt;p&gt;You can do more analysis on the data, the more we question the data, the better we know about it. You can create more and more Hypothesis to verify more about your objective. You can also create a weather prediction model; using this data to train and test your model. It’s all up to you. That is one of the best thing about exploratory data analysis.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>